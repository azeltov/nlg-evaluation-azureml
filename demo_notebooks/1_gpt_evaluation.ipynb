{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate LLMs Using LLMs**\n",
    "\n",
    "### Overview\n",
    "In this demonstration, you will evaluate the output of a simple chat LLM using an LLM. In this case, you will use GPT35-turbo-instruct to evaluate chat outputs for the following metrics:\n",
    "1. Fluency = \n",
    "2. Coherence = \n",
    "3. Relevance = \n",
    "\n",
    "After utilizing Azure PromptFlow to generate and evaluate chat responses, this notebook will take a deeper look at the results.\n",
    "\n",
    "### Go Deeper\n",
    "The inspiration for this demonstration comes from the paper [Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?](https://ar5iv.labs.arxiv.org/html/2309.07462).\n",
    "\n",
    "### Prerequisites\n",
    "Ensure that your environment is setup by completing the steps outlines in [0_setup.ipynb](./0_setup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Sample Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id = os.environ.get('SUBSCRIPTION_ID'),\n",
    "    resource_group_name = os.environ.get('RESOURCE_GROUP_NAME'),\n",
    "    workspace_name = os.environ.get('WORKSPACE_NAME'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload input/simplechat_sample.csv to the workspace\n",
    "\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import time\n",
    "\n",
    "# update the 'my_path' variable to match the location of where you downloaded the data on your\n",
    "# local filesystem\n",
    "\n",
    "local_path = \"../data/inputs/simplechat_sample_input.csv\"\n",
    "# set the version number of the data asset to the current UTC time\n",
    "v1 = time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime())\n",
    "\n",
    "\n",
    "my_data = Data(\n",
    "    name=\"simplechat-sample-inputs\",\n",
    "    version=v1,\n",
    "    description=\"Sample inputs for simple chat flow\",\n",
    "    path=local_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "# create data asset\n",
    "ml_client.data.create_or_update(my_data)\n",
    "\n",
    "print(f\"Data asset created. Name: {my_data.name}, version: {my_data.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Simple Chat Prompt Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Analyze Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
