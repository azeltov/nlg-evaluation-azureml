{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate LLMs Using LLMs**\n",
    "\n",
    "### Overview\n",
    "In this demonstration, you will evaluate the output of a simple chat LLM using an LLM. In this case, you will use GPT35-turbo-instruct to evaluate chat outputs for the following metrics:\n",
<<<<<<< HEAD
    "1. **Fluency** - Measures how grammatically and linguistically correct the model's predicted answer is.\n",
    "2. **Coherence** - Measures the quality of all sentences in a model's predicted answer and how they fit together naturally.\n",
    "3. **Relevance** - Measures how relevant the model's predicted answers are to the questions asked.\n",
    "\n",
    "After utilizing Azure PromptFlow to generate and evaluate chat responses, this notebook will take a deeper look at the results.\n",
    "\n",
    " **_Go Deeper_**  \n",
    " The inspiration for this demonstration comes from the paper [Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?](https://ar5iv.labs.arxiv.org/html/2309.07462).\n",
=======
    "1. Fluency = \n",
    "2. Coherence = \n",
    "3. Relevance = \n",
    "\n",
    "After utilizing Azure PromptFlow to generate and evaluate chat responses, this notebook will take a deeper look at the results.\n",
    "\n",
    "### Go Deeper\n",
    "The inspiration for this demonstration comes from the paper [Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?](https://ar5iv.labs.arxiv.org/html/2309.07462).\n",
>>>>>>> e4f4274243201fb61b6959f4b03ae547d8efee24
    "\n",
    "### Prerequisites\n",
    "Ensure that your environment is setup by completing the steps outlines in [0_setup.ipynb](./0_setup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Sample Input Data"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import os\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id = os.environ.get('SUBSCRIPTION_ID'),\n",
    "    resource_group_name = os.environ.get('RESOURCE_GROUP_NAME'),\n",
    "    workspace_name = os.environ.get('WORKSPACE_NAME'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading simple_chat_sample_inputs.csv\u001b[32m (< 1 MB): 100%|██████████| 1.67k/1.67k [00:00<00:00, 21.6kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data asset created. Name: simple-chat-sample-inputs, version: 2023.12.11.213914\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import time\n",
    "\n",
    "local_path = \"../data/inputs/simple_chat_sample_inputs.csv\"\n",
    "# set the version number of the data asset to the current UTC time\n",
    "v1 = time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime())\n",
    "\n",
    "\n",
    "my_data = Data(\n",
    "    name=\"simple-chat-sample-inputs\",\n",
    "    version=v1,\n",
    "    description=\"Sample inputs for simple chat flow\",\n",
    "    path=local_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "# create data asset\n",
    "ml_client.data.create_or_update(my_data)\n",
    "\n",
    "print(f\"Data asset created. Name: {my_data.name}, version: {my_data.version}\")"
   ]
  },
  {
=======
>>>>>>> e4f4274243201fb61b6959f4b03ae547d8efee24
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Simple Chat Prompt Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Analyze Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
<<<<<<< HEAD
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
=======
  "language_info": {
   "name": "python"
>>>>>>> e4f4274243201fb61b6959f4b03ae547d8efee24
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
