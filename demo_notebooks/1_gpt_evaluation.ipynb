{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate LLMs Using LLMs**\n",
    "\n",
    "### Overview\n",
    "In this demonstration, you will evaluate the output of a simple chat LLM using an LLM. In this case, you will use GPT35-turbo-instruct to evaluate chat outputs for the following metrics:\n",
    "1. Fluency = \n",
    "2. Coherence = \n",
    "3. Relevance = \n",
    "\n",
    "After utilizing Azure PromptFlow to generate and evaluate chat responses, this notebook will take a deeper look at the results.\n",
    "\n",
    "### Go Deeper\n",
    "The inspiration for this demonstration comes from the paper [Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?](https://ar5iv.labs.arxiv.org/html/2309.07462).\n",
    "\n",
    "### Prerequisites\n",
    "Ensure that your environment is setup by completing the steps outlines in [0_setup.ipynb](./0_setup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Sample Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Simple Chat Prompt Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Analyze Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
