{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How Accurate are GPT Metrics Compared to Human Analysis?**\n",
    "\n",
    "### Overview\n",
    "In this demonstration, you will compare GPT4s assessment of Coherence against human graded samples. This demo uses the SummEval dataset, SummEval contains LLM generated summarizations that have been human graded on Coherence. Coherence measures the quality of all sentences in a model's predicted answer and how they fit together naturally.\n",
    "\n",
    "You will utilize Azure PromptFlow to generate Coherence evaluations using GPT4. Using PromptFlow you will use two Coherence prompts - a standard template and a template using 'emotion' as referenced in the linked paper below. You will then use this notebooks to compare and analyze the GPT4, GPT4 with emotion, and human graded coherence scores.\n",
    "\n",
    " **_Go Deeper_**  \n",
    "- [Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?](https://ar5iv.labs.arxiv.org/html/2309.07462)\n",
    "- [GptEval: NLG Evaluation using Gpt-4 with Better Human Alignment](https://ar5iv.labs.arxiv.org/html/2303.16634)\n",
    "- [EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus](https://arxiv.org/pdf/2307.11760v3.pdf)\n",
    "- SummEval: Re-evaluating Summarization Evaluation [[Paper]](https://arxiv.org/pdf/2007.12626.pdf) / [[Repository]](https://github.com/Yale-LILY/SummEval#data)  \n",
    "  \n",
    "**_Prerequisites_**  \n",
    "\n",
    "Ensure that your environment is setup by completing the steps outlines in [0_setup.ipynb](./0_setup.ipynb)  \n",
    "_Optional_: For an overview of GPT based metrics, please see [1_gpt_evaluation.ipynb](./1_gpt_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Examine Input Data\n",
    "The SummEval dataset provides human labeled analysis on LLM generated summaries.The annotations include summaries generated by 16 models from 100 source news articles (~15000 examples in total). You will use a subset of ~250 samples.\n",
    "Each of the summaries was annotated by 5 indepedent crowdsource workers and 3 independent experts (8 annotations in total).\n",
    "Summaries were evaluated across 4 dimensions: coherence, consistency, fluency, relevance.\n",
    "Each source news article comes with the original reference from the CNN/DailyMail dataset and 10 additional crowdsources reference summaries.  \n",
    "  \n",
    "Take a look at the data [summEval_human_labeled.jsonl](../data/inputs/summEval_human_labeled.jsonl)  \n",
    "\n",
    "_Note: Human labels are subjective and are **not** perfect, comparison is for benchmarking purposes and should be considered separate from result quality_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run Evaluation Pipeline\n",
    "\n",
    "**IMPORTANT:** Be sure to analyze and expirament with the [gpt_eval_benchmark](../src/promptflow/evaluation_flows/gpt_eval_benchmark/) PromptFlow used in this step.  \n",
    "  \n",
    "  _This step may take upt to 10 minutes to complete_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-13 14:42:12,589][promptflow._sdk._submitter.run_submitter][WARNING] - 2 out of 249 runs failed in batch run. Please check out /home/zacksoenen/.promptflow/.runs/gpt_eval_benchmark_variant_0_20231213_143631_407357 for more details.\n"
     ]
    }
   ],
   "source": [
    "from promptflow import PFClient\n",
    "\n",
    "# PFClient can help manage your runs and connections.\n",
    "pf = PFClient()\n",
    "\n",
    "# Define Flows and Data\n",
    "eval_flow = \"../src/promptflow/evaluation_flows/gpt_eval_benchmark\" # set flow directory\n",
    "data = \"../data/inputs/summEval_human_labeled_subset.jsonl\" # set the data file\n",
    "\n",
    "# Run evaluation flow to evaluate chat results\n",
    "eval_run = pf.run(\n",
    "    flow=eval_flow,\n",
    "    data=data,\n",
    "    stream=False,\n",
    "    column_mapping={  # map the url field from the data to the url input of the flow\n",
    "      \"expert_annotations\": \"${data.expert_annotations}\",\n",
    "      \"turker_annotations\": \"${data.turker_annotations}\",\n",
    "      \"response\": \"${data.decoded}\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful Documentation:  \n",
    "[Run and Evaluate a PromptFlow](https://microsoft.github.io/promptflow/how-to-guides/run-and-evaluate-a-flow/index.html)  \n",
    "[PFClient Documentation](https://microsoft.github.io/promptflow/reference/python-library-reference/promptflow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Analyze Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>gpt_coherence</th>\n",
       "      <th>gpt_coherence_w_emotion</th>\n",
       "      <th>expert_coherence</th>\n",
       "      <th>turker_coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paul merson was brought on with only seven min...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paul merson has restarted his row with andros ...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paul merson has restarted his row with andros ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paul merson has restarted his row with andros ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paul merson has restarted his row with andros ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>world no 1 williams said her struggle to beat ...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>serena williams said her struggle to beat sara...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>twice french open champion serena williams sai...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>serena williams beat sara errani 4-6 7-6(3) 6-...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Serena Williams won 6-3 6-3 against Sara Erran...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              response  gpt_coherence  \\\n",
       "0    paul merson was brought on with only seven min...              3   \n",
       "1    paul merson has restarted his row with andros ...              4   \n",
       "2    paul merson has restarted his row with andros ...              5   \n",
       "3    paul merson has restarted his row with andros ...              4   \n",
       "4    paul merson has restarted his row with andros ...              5   \n",
       "..                                                 ...            ...   \n",
       "242  world no 1 williams said her struggle to beat ...              4   \n",
       "243  serena williams said her struggle to beat sara...              5   \n",
       "244  twice french open champion serena williams sai...              5   \n",
       "245  serena williams beat sara errani 4-6 7-6(3) 6-...              5   \n",
       "246  Serena Williams won 6-3 6-3 against Sara Erran...              4   \n",
       "\n",
       "     gpt_coherence_w_emotion  expert_coherence  turker_coherence  \n",
       "0                          3                 1                 3  \n",
       "1                          5                 2                 2  \n",
       "2                          5                 2                 4  \n",
       "3                          4                 2                 5  \n",
       "4                          5                 3                 2  \n",
       "..                       ...               ...               ...  \n",
       "242                        5                 2                 5  \n",
       "243                        5                 4                 1  \n",
       "244                        5                 4                 4  \n",
       "245                        5                 2                 3  \n",
       "246                        4                 4                 3  \n",
       "\n",
       "[247 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_data = \"../data/outputs/gpt_benchmark_results.json\"\n",
    "\n",
    "output_df = pd.read_json(output_data)\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: General historgram overlay fpr each metric\n",
    "\n",
    "#TODO: Distribution of gpt_coherence / w_emotion variance against expert\n",
    "\n",
    "#TODO: Distribution of gpt_coherence / w_emotion variance against turker (crowdsourced)\n",
    "\n",
    "#TODO: Overall aggregate metrics for accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
