{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How Accurate are GPT Metrics Compared to Human Analysis?**\n",
    "\n",
    "### Overview\n",
    "In this demonstration, you will compare GPT4s assessment of Coherence against human graded samples. This demo uses the SummEval dataset, SummEval contains LLM generated summarizations that have been human graded on Coherence. Coherence measures the quality of all sentences in a model's predicted answer and how they fit together naturally.\n",
    "\n",
    "You will utilize Azure PromptFlow to generate Coherence evaluations using GPT4. Using PromptFlow you will use two Coherence prompts - a standard template and a template using 'emotion' as referenced in the linked paper below. You will then use this notebooks to compare and analyze the GPT4, GPT4 with emotion, and human graded coherence scores.\n",
    "\n",
    " **_Go Deeper_**  \n",
    "- [Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?](https://ar5iv.labs.arxiv.org/html/2309.07462)\n",
    "- [GptEval: NLG Evaluation using Gpt-4 with Better Human Alignment](https://ar5iv.labs.arxiv.org/html/2303.16634)\n",
    "- [EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus](https://arxiv.org/pdf/2307.11760v3.pdf)\n",
    "- SummEval: Re-evaluating Summarization Evaluation [[Paper]](https://arxiv.org/pdf/2007.12626.pdf) / [[Repository]](https://github.com/Yale-LILY/SummEval#data)\n",
    "### Prerequisites\n",
    "Ensure that your environment is setup by completing the steps outlines in [0_setup.ipynb](./0_setup.ipynb)  \n",
    "_Optional_: For an overview of GPT based metrics, please see [1_gpt_evaluation.ipynb](./1_gpt_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Examine Input Data\n",
    "The SummEval dataset provides human labeled analysis on LLM generated summaries.The annotations include summaries generated by 16 models from 100 source news articles (~156000 examples in total). You will use a subset of ~1000 samples.\n",
    "Each of the summaries was annotated by 5 indepedent crowdsource workers and 3 independent experts (8 annotations in total).\n",
    "Summaries were evaluated across 4 dimensions: coherence, consistency, fluency, relevance.\n",
    "Each source news article comes with the original reference from the CNN/DailyMail dataset and 10 additional crowdsources reference summaries.  \n",
    "  \n",
    "Take a look at the data [summEval_human_labeled.jsonl](../data/inputs/summEval_human_labeled.jsonl)  \n",
    "\n",
    "_Note: Human labels are subjective and are **not** perfect, comparison is for benchmarking purposes and should be considered separate from result quality_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run Evaluation Pipeline\n",
    "\n",
    "**IMPORTANT:** Be sure to analyze and expirament with the [gpt_eval_benchmark](../src/promptflow/evaluation_flows/gpt_eval_benchmark/) PromptFlow used in this step.  \n",
    "  \n",
    "  _This step may take upt to 10 minutes to complete_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from promptflow import PFClient\n",
    "\n",
    "# PFClient can help manage your runs and connections.\n",
    "pf = PFClient()\n",
    "\n",
    "# Define Flows and Data\n",
    "eval_flow = \"../src/promptflow/evaluation_flows/gpt_eval_benchmark\" # set flow directory\n",
    "data = \"../data/inputs/summEval_human_labeled.jsonl\" # set the data file\n",
    "\n",
    "# Run evaluation flow to evaluate chat results\n",
    "eval_run = pf.run(\n",
    "    flow=eval_flow,\n",
    "    data=data,\n",
    "    stream=False,\n",
    "    column_mapping={  # map the url field from the data to the url input of the flow\n",
    "      \"expert_annotaions\": \"${data.expert_annotations}\",\n",
    "      \"turker_annotations\": \"${data.turker_annotations}\",\n",
    "      \"response\": \"${data.decoded}\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
