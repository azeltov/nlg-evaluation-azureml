{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLG Hallucination Detection**\n",
    "\n",
    "Hallucinations are when a model confidently presents an incorrect or unfactual response to an end user. They are the most popular critizism and risk assosiated with LLMs and natural language generation. Mitigating Halluncinations is critical for user safety and application adoption.\n",
    "\n",
    "There are two types of halluncinations...\n",
    "1. Open-domain: Flase claims that contradict a ground truth\n",
    "2. Closed-domain: Deviations from the context of a specific reference text (i.e. RAG)  \n",
    "  \n",
    "...and there are two categories of hallucination prevention:\n",
    "1. Mitigation: Prevention methods before a response is generated using techniques like Prompt Engineering (\"Say I don't know\") or RAG\n",
    "2. Detection: Analyzing the response after it is generated. These can be used in-line to prevent a hallucenated response from being presented to a user\n",
    "\n",
    "Most state-of-the-art detection methods to date involve sampling multiple responses from you chat and analyzing the entire response set to identify hallucination risk. [How to Perform Hallucination Detection by Mark Chen](https://towardsdatascience.com/how-to-perform-hallucination-detection-for-llms-b8cb8b72e697) provides a great overview of the latest and most effective methods.\n",
    "\n",
    "**This notebook** will use Azure PromptFlow to implement 4 techniques of **_halluncination  detection_** inline with chat or Q&A response:\n",
    "1. **Hughes Hallucination Evaluation Model**: Open source fine-tuned deberta-v3-base model for hallucination detection. \n",
    "2. **Natural Language Inference (NLI) Contradiction Score**: Use the SelfCheckGPT framework + nli-deberta-v3-base OOS language model to evaluate sample contradiction\n",
    "3. **Self-Consistency Chain-of-Thought (CoT)**: Use SelfCheckGPT framework + mistral-7B-Instruct-v0.2 OOS language model to evaluate sample consistency\n",
    "4. **ChainPoll**: Make multiple calls to GPT4 to poll for halluncination likelihood\n",
    "  \n",
    "\n",
    "**_Go Deeper_**  \n",
    "    \n",
    "- HHEM: [Hughes Hallucination Evaluation Model (HHEM)](https://huggingface.co/vectara/hallucination_evaluation_model)  \n",
    "- Contradiction Scoring: [Self-contradtctory Hallucinations of Large Language Models](https://arxiv.org/pdf/2305.15852v1.pdf) & [With a Little Push, NLLI Models can Robustly and Efficiently Predict Faithfulness](https://arxiv.org/pdf/2305.16819.pdf)  \n",
    "- Consistency Scoring: [Semantic Consistency for Assuring Reliability of LLMs](https://arxiv.org/pdf/2308.09138.pdf)\n",
    "- ChainPoll: [ChainPoll: A High Efficacy Method for LLM Hallucination Detection](https://arxiv.org/pdf/2310.18344v1.pdf)  \n",
    "- Self-Check GPT: Zero-Resource Black-Box Hallucination Detection for Generative LLMs ([Paper](https://arxiv.org/pdf/2303.08896.pdf)/[Code](https://github.com/potsawee/selfcheckgpt))  \n",
    "  \n",
    "**_How do I know my detection is strong?_**\n",
    "- To test your detection framework you can utilize labeled OOS datasets such as...\n",
    "  - HaluEval ([Paper](https://arxiv.org/abs/2305.11747)/[Code](https://github.com/RUCAIBox/HaluEval)) (closed-domain)\n",
    "  - [TruthfulQA](https://github.com/sylinrl/TruthfulQA) (open-domain)\n",
    "- Or bring your own labeled dataset that is tailored to your use case\n",
    "  \n",
    "**_Prerequisites_**  \n",
    "  \n",
    "- Ensure that your environment is setup by completing the steps outlines in [0_setup.ipynb](./0_setup.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
