{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluating Groundedness using LLMs**\n",
    "\n",
    "### Overview\n",
    "In this demonstration, you will evaluate the groundedness of the context for a RAG architecture pattern. RAG (Retrieval Augmented Generation) is a common method used to improve LLM results and reduce hallucnations by 'grounding' the LLM resposne using information retrieval. There are generally to types og \"Groundedness\" metrics:  \n",
    "1. **Question Groundedness** - Measures the quality of the search index. \"How good is the data being retrieved for a given question?\"\n",
    "2. **Answer Groundedness** - Measures the quality of LLM response. \"How good is my LLM at using the data being retrieved?\"\n",
    "\n",
    "After utilizing Azure PromptFlow to generate and evaluate RAG chat responses, this notebook will take a deeper look at the results.\n",
    "\n",
    " **_Go Deeper_**  \n",
    "- [Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?](https://ar5iv.labs.arxiv.org/html/2309.07462)\n",
    "- [GptEval: NLG Evaluation using Gpt-4 with Better Human Alignment](https://ar5iv.labs.arxiv.org/html/2303.16634)\n",
    "  \n",
    "**_Prerequisites_**  \n",
    "  \n",
    "Ensure that your environment is setup by completing the steps outlines in [0_setup.ipynb](./0_setup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Sample Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "import os\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id = os.environ.get('SUBSCRIPTION_ID'),\n",
    "    resource_group_name = os.environ.get('RESOURCE_GROUP_NAME'),\n",
    "    workspace_name = os.environ.get('WORKSPACE_NAME'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import time\n",
    "\n",
    "#TODO create sample data\n",
    "local_path = \"XXX\"\n",
    "# set the version number of the data asset to the current UTC time\n",
    "v1 = time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime())\n",
    "\n",
    "\n",
    "my_data = Data(\n",
    "    name=\"simple-chat-sample-inputs\",\n",
    "    version=v1,\n",
    "    description=\"Sample inputs for simple chat flow\",\n",
    "    path=local_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "# create data asset\n",
    "ml_client.data.create_or_update(my_data)\n",
    "\n",
    "print(f\"Data asset created. Name: {my_data.name}, version: {my_data.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Simple Chat & GPT Evaluation PromptFlow Jobs\n",
    "In this section you will run a [sample RAG chat](../src/promptflow/sample_chat_flows/simplrag_chat) against a small sample dataset  \n",
    "\n",
    "Then, as part of the same job, you will evaluate the GPT metrics above using an [evaluation PromptFlow](../src/promptflow/evaluation_flows/rag_gpt_eval/)\n",
    "\n",
    "Both the simple chat and the evaluation utilize the AOAI connection established during setup and cooresponding GPT4 deployment\n",
    "\n",
    "##### **IMPORTANT**: _Please take a moment to analyze in depth the Simple Chat, Evaluation Flow, and the sample dataset linked above_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import PFClient\n",
    "\n",
    "# PFClient can help manage your runs and connections.\n",
    "pf = PFClient()\n",
    "\n",
    "# Define Flows and Data\n",
    "simple_chat_flow = \"../src/promptflow/sample_chat_flows/rag_chat\" # set the flow directory\n",
    "eval_flow = \"../src/promptflow/evaluation_flows/rag_gpt_eval\" # set flow directory\n",
    "# TODO data = \"../data/inputs/simple_chat_sample_inputs.csv\" # set the data file\n",
    "\n",
    "# Run chat flow to generate chat results\n",
    "chat_run = pf.run(\n",
    "    flow=simple_chat_flow,\n",
    "    data=data,\n",
    "    stream=False,\n",
    "    column_mapping={  # map the url field from the data to the url input of the flow\n",
    "      \"input\": \"${data.input}\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run evaluation flow to evaluate chat results\n",
    "eval_run = pf.run(\n",
    "    flow=eval_flow,\n",
    "    data=data,\n",
    "    run=chat_run,\n",
    "    stream=False,\n",
    "    column_mapping={  # map the url field from the data to the url input of the flow\n",
    "      \"question\": \"${data.input}\",\n",
    "      \"response\": \"${run.outputs.output}\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  View Results  \n",
    "To view outputs in detail analyze the [output data](../data/outputs/rag_eval_results.json) directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
